{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 2 : End-to-End Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look at the big picture.\n",
    "2. Get the data.\n",
    "3. Discover and visualize the data to gain insights.\n",
    "4. Prepare the data for Machine Learning algorithms.\n",
    "5. Select a model and train it.\n",
    "6. Fine-tune your model.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines:\n",
    "A sequence of data processing components is called a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. The first question to ask your boss is what exactly is the business objective; building a model is probably not the end goal. How does the company expect to use and benefit from this model?\n",
    "<br>\n",
    "2. what the current solution looks like (if any). It will often give you a reference performance, as well as insights on how to solve the problem.\n",
    "<br>\n",
    "\n",
    "<b>frame the problem: is it supervised, unsupervised, or Reinforcement Learning? Is it a classification task, a regression task, or something else? batch learning or online learning techniques?</b><br>\n",
    "Select a `performance measure`---> `RMSE`, Root mean square error measures standard deviation of the errors the system makes in its predictions.`RMSE\n",
    "equal to 50,000 means that about 68% of the system’s predictions fall within 50,000 of the actual value, and about 95% of the predictions fall within 100,000` <br>\n",
    "other measures are Mean Absolute Error, Root Mean Sum of Squares,Sum of Absolutes\n",
    "\n",
    "3. Check Assumptions\n",
    "4. Get data\n",
    "5. plot and explore:\n",
    "\n",
    "Now Caping of data(fixed according to the client needs), scales of data(fixed by the feature scaling) and being tail heavy(fixed by transforming them to bell-shaped distributions).<br>\n",
    "\n",
    "The sampling of the data while creating test and train sets is necessary.<br>\n",
    "`stratified sampling`: the population is divided into homogeneous subgroups called strata,\n",
    "and the right number of instances is sampled from each stratum to guarantee that the\n",
    "test set is representative of the overall population.\n",
    "\n",
    "<p>\n",
    "    \n",
    "6. Look for correlations\n",
    "7. Experiment with atrribute combinations\n",
    "8. Data Cleaning:\n",
    "dealing with N/A values, `get rid of corresponding values,rid of the attribute or replace each one with some values like zero,mean,median...`<br><p>\n",
    "    \n",
    "sklearn has `Imputer in sklearn.preprocessing` <br>\n",
    "<b>Fit and Transform the imputer object.</b><br>\n",
    "`Estimators`:Any object that can estimate some parameters based on a dataset is called an estimator (e.g., an imputer is an estimator). The estimation itself is performed by the fit() method.<br>\n",
    "`Transformers`: Some estimators (such as an imputer ) can also transform a dataset; these are called transformers. Once again, the API is quite simple: the transformation is performed by the transform() method.<br>\n",
    "`Predictors`: some estimators are capable of making predictions given a dataset; they are called predictors.<br>\n",
    "`Inspection`: estimator’s hyperparameters are accessible directly via public\n",
    "instance variables (e.g., imputer.strategy ) say `imputer.statistics_` gives the values.<br>\n",
    "<p>\n",
    "    \n",
    "    \n",
    "9. Handling Text and Categorical Attributes:\n",
    "`LabelEncoder` for text and  `OneHotEncoder` for categorical ones and `LabelBinarizer` for doing the onehotencoding and converting into numpy array directly.<br>\n",
    "\n",
    "<p>\n",
    "    \n",
    "10. Feature Scaling:\n",
    "Common ways min-max(Normalization) and Standarization.<br>\n",
    "\n",
    "min-max---> values are shifted and rescaled so that they end up ranging from 0 to 1.<br>\n",
    "We do this by subtract‐\n",
    "ing the min value and dividing by the max minus the min. Scikit-Learn provides a\n",
    "transformer called `MinMaxScaler` for this. It has a `feature_range` hyperparameter\n",
    "that lets you change the range if you don’t want 0–1 for some reason.\n",
    "<p>\n",
    "    \n",
    "    \n",
    "`Standardization` is quite different: first it subtracts the mean value (so standardized\n",
    "values always have a zero mean), and then it divides by the variance so that the result‐\n",
    "ing distribution has unit variance.<br>\n",
    "Scikit-Learn provides a transformer called `StandardScaler` for standardization. It is less affected by outliers than min-max scaling.<br>\n",
    "\n",
    "11. These make the `Transformation Pipelines`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline’s `fit()` method, it calls `fit_transform()` sequentially on\n",
    "all transformers, passing the output of each call as the parameter to the next call, until\n",
    "it reaches the final estimator, for which it just calls the fit() method.<br>\n",
    "<b>We can make different Subpipelines to make for different tasks where we can even use our functions as parameters to achieve the tasks.</b><br><p>\n",
    "        \n",
    "12. Select and Train a Model:\n",
    "We select a model and then train the model with our training data.<br>\n",
    "Evaluate the model on the test set.<br>\n",
    "See if there is under fitting with `sklearn.metrics.mean_squared_error() y_train and predictions.` <br>\n",
    "if the `sqrt` of above is more, the model is underfitting, we increase the complexity of the model and thus select the decisiontree regressor(`sklearn.tree.DecisionTreeRegressor`) model and train on it. calculate the mean_squared_error again and see if it overfits(it does overfit in this case). <br><p>\n",
    "    \n",
    "    \n",
    "13. Better Evaluation Using Cross-Validation:\n",
    "`sklearn.model_selection.cross_val_score(model, X_train,y_train,scoring='neg_mean_squared_error',cv=10)` crossvalidates the model with `K-fold cross validation technique.` <br>\n",
    "<p>\n",
    "    \n",
    "`mean` of scores is worse than the linear regression. So yes it `overfits`.<br>\n",
    "<p>\n",
    "    \n",
    "    \n",
    "`sklearn.ensemble.RandomForestRegressor` is used now.<br>\n",
    "Building a model on top of many\n",
    "other models is called `Ensemble Learning`, and it is often a great way to push ML algo‐\n",
    "rithms even further.<br>\n",
    "\n",
    "<p>\n",
    "    \n",
    "    \n",
    "    \n",
    "Before tuning the hyperparameters experiment with other models and see cross-validation scores with predictions. `We can save sklearn models by using Python's pickle module`.<br>\n",
    "OR with:<br>\n",
    "`sklearn.externals.joblib , which is more efficient at serializing large NumPy arrays:`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(my_model, \"my_model.pkl\")\n",
    "# and later...\n",
    "my_model_loaded = joblib.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Fine-Tune Your Model:\n",
    "`Scikit-Learn’s GridSearchCV` to fine tune the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "    ]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ways to finetune your model is `Randomized search` or `Ensemble Methods` <br>\n",
    "see the `feature importances` with `grid_search.best_estimator_.feature_importances_`<br>\n",
    "<p>\n",
    "    \n",
    "14. Now evaluate the system on Test Set.<br>\n",
    "To present your solution (high‐\n",
    "lighting what you have learned, what worked and what did not, what assumptions\n",
    "were made, and what your system’s limitations are), document everything, and create\n",
    "nice presentations with clear visualizations and easy-to-remember statements (e.g.,\n",
    "“the median income is the number one predictor of housing prices”).\n",
    "\n",
    "15. Launch, Monitor, and Maintain Your System\n",
    "You need to get your solution ready for produc‐\n",
    "tion, in particular by plugging the production input data sources into your system and writing tests.<br>\n",
    "<p>\n",
    "    \n",
    "`Monitoring` is important for the linve performance and to catch the sudden breakage, also the models `rot` over time, unless th models are regularized on fresh data.<br>\n",
    "\n",
    "Human pipelines may be required to monitor. Poor quality `signals` have to be prevented(maintaining the sensors,alerts for the bad signal and performance drops.)<br><p>\n",
    "    \n",
    "    \n",
    "Mostly automate these alerts and catching bad singnals thingy.<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
